{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a98fc1-8bf6-4e15-9755-1166c3578369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "# import weaviate.classes.config as wc\n",
    "# import weaviate.classes.query as wq\n",
    "from weaviate.util import generate_uuid5\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35910d9-233d-4558-b8d8-c530f8776258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0feddf-6c81-4133-a4a2-aee5635779f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    \"openie.affinity_probability_cap\": 2 / 3,\n",
    "    \"openie.triple.strict\": False\n",
    "}\n",
    "IEclient = StanfordOpenIE(properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6735-1bc5-4cae-95ad-a6d3698506dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a66a-d049-4157-8eea-a0caf8d423fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(predictions, batch_size: int = 16, add_start_token: bool = True, max_length=512):\n",
    "        if torch.cuda.is_available() == True:\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "    \n",
    "        # if batch_size > 1 (which generally leads to padding being required), and\n",
    "        # if there is not an already assigned pad_token, assign an existing\n",
    "        # special token to also be the padding token\n",
    "        if tokenizer.pad_token is None and batch_size > 1:\n",
    "            existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "            # check that the model already has at least one special token defined\n",
    "            assert (\n",
    "                len(existing_special_tokens) > 0\n",
    "            ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "            # assign one of the special tokens to also be the pad token\n",
    "            tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "        if add_start_token and max_length:\n",
    "            # leave room for <BOS> token to be added:\n",
    "            assert (\n",
    "                tokenizer.bos_token is not None\n",
    "            ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "            max_tokenized_len = max_length - 1\n",
    "        else:\n",
    "            max_tokenized_len = max_length\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            predictions,\n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True if max_tokenized_len else False,\n",
    "            max_length=max_tokenized_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        encoded_texts = encodings[\"input_ids\"]\n",
    "        attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "        # check that each input is long enough:\n",
    "        if add_start_token:\n",
    "            assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "        else:\n",
    "            assert torch.all(\n",
    "                torch.ge(attn_masks.sum(1), 2)\n",
    "            ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "        ppls = []\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        for start_index in range(0, len(encoded_texts), batch_size):\n",
    "            end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "            encoded_batch = encoded_texts[start_index:end_index]\n",
    "            attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "            if add_start_token:\n",
    "                bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "                encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "                attn_mask = torch.cat(\n",
    "                    [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "                )\n",
    "\n",
    "            labels = encoded_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "            perplexity_batch = torch.exp(\n",
    "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                / shift_attention_mask_batch.sum(1)\n",
    "            )\n",
    "\n",
    "            ppls += perplexity_batch.tolist()\n",
    "\n",
    "        return {\"perplexities\": ppls, \"mean_perplexity\": np.mean(ppls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477cb97-d024-4dd5-bb86-681cfccdeb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples_ie(text):\n",
    "    res = [x for x in IEclient.annotate(text)]\n",
    "    temp = [tuple(x.values()) for x in res]\n",
    "\n",
    "    current = defaultdict(list)\n",
    "    for t in temp:\n",
    "        current[(t[0], t[1])].append(t)\n",
    "\n",
    "    final = []\n",
    "    for t in temp:\n",
    "        s = \"{} | {} | {}\".format(t[0], t[1], t[2])\n",
    "        if s not in final:\n",
    "            final.append(s.replace(\"_\", \" \"))\n",
    "    \n",
    "    lsh = MinHashLSH(threshold=0.4, num_perm=128)\n",
    "    minhashes = {}\n",
    "    for i, f in enumerate(final):\n",
    "        minhash = MinHash(num_perm=128)\n",
    "        for d in ngrams(f, 3):\n",
    "            minhash.update(\"\".join(d).encode('utf-8'))\n",
    "        lsh.insert(i, minhash)\n",
    "        minhashes[i] = minhash\n",
    "\n",
    "    matches = {}\n",
    "    for x, y in zip(final, minhashes):\n",
    "        matches[x] = [final[z] for z in lsh.query(minhashes[y]) if z != y] \n",
    "\n",
    "    clusters = []\n",
    "    covered = []\n",
    "    for m in sorted(matches, key=lambda x: len(matches[x]), reverse=True):\n",
    "        if m not in covered and len(matches[m]) > 0:\n",
    "            clusters.append(matches[m])\n",
    "            covered.extend(matches[m])\n",
    "\n",
    "    clean = [x.replace(\" | \", \" \") for x in covered]\n",
    "    if len(clean) == 0:\n",
    "        return []\n",
    "    ppls = dict(zip(covered, compute_ppl(predictions=clean, batch_size=128)[\"perplexities\"]))\n",
    "\n",
    "    best = []\n",
    "    for c in clusters:\n",
    "        scores = [ppls[x] for x in c]\n",
    "        imin = np.argmin(scores)\n",
    "        best.append(c[imin])\n",
    "\n",
    "    ordered = []\n",
    "    for f in final:\n",
    "        if f in best:\n",
    "            ordered.append(f)\n",
    "    return ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09bfc0-0edd-49bf-9e2f-a03c0461761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1616c-04e3-4397-9eb9-0f407369cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local() # must have Weaviate installed and running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b2b56-f148-46e0-bb9d-213283181dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once!\n",
    "# client.collections.create(\n",
    "#     name=\"Triples\",\n",
    "#     properties=[\n",
    "#         wc.Property(name=\"text\", data_type=wc.DataType.TEXT),\n",
    "#     ],\n",
    "#     vectorizer_config=wc.Configure.Vectorizer.none(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c2e08-d79f-4d92-831e-36675e487e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = client.collections.get(\"Triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f4f9a-b5cb-49c0-9669-f713853327ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb\", \"sample-10BT\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c95ddb-3019-42cc-af52-b8e177c5d77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "idx = 0\n",
    "total = 0\n",
    "for row in dataset:\n",
    "    if idx == 10000000:\n",
    "        break\n",
    "    \n",
    "    if idx % 100000 == 0 and idx != 0:\n",
    "        print(\"{} rows processed.\".format(idx))\n",
    "\n",
    "    if len(row[\"text\"]) > 10000 or row[\"language\"] != \"en\":\n",
    "        continue\n",
    "    \n",
    "    all_data.append(row[\"text\"])\n",
    "\n",
    "    if len(all_data) >= 1000:\n",
    "        print(\"Extracting triples...\")\n",
    "        res = [get_triples_ie(x) for x in tqdm(all_data)]\n",
    "        tri = []\n",
    "        for r in res:\n",
    "            tri.extend(r)\n",
    "        \n",
    "        total += len(tri)\n",
    "        print(\"Inserting {} triples...\".format(len(tri)))\n",
    "        embeddings = embedding_model.encode(tri, task=\"text-matching\", truncate_dim=32, max_length=64)\n",
    "\n",
    "        with triples.batch.dynamic() as batch:\n",
    "            for i, t in enumerate(tri):\n",
    "                obj = {\"text\":t}\n",
    "                vector = embeddings[i]\n",
    "        \n",
    "                batch.add_object(\n",
    "                    properties=obj,\n",
    "                    uuid=generate_uuid5(obj),\n",
    "                    vector=vector\n",
    "                )\n",
    "        print(\"Finished. Total: {}\".format(total))\n",
    "        del tri[:]\n",
    "        del tri\n",
    "        del all_data[:]\n",
    "        all_data = []\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "print(\"Extracting triples...\")\n",
    "res = [get_triples_ie(x) for x in tqdm(all_data)]\n",
    "tri = []\n",
    "for r in res:\n",
    "    tri.extend(r)\n",
    "\n",
    "total += len(tri)\n",
    "print(\"Inserting {} triples...\".format(len(tri)))\n",
    "embeddings = embedding_model.encode(tri, task=\"text-matching\", truncate_dim=32, max_length=64)\n",
    "\n",
    "with triples.batch.dynamic() as batch:\n",
    "    for i, t in enumerate(tri):\n",
    "        obj = {\"text\":t}\n",
    "        vector = embeddings[i]\n",
    "\n",
    "        batch.add_object(\n",
    "            properties=obj,\n",
    "            uuid=generate_uuid5(obj),\n",
    "            vector=vector\n",
    "        )\n",
    "print(\"Finished. Total: {}\".format(total))\n",
    "del tri[:]\n",
    "del tri\n",
    "del all_data[:]\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3559d04-6bb8-4ef9-b8e2-4e65e5ead1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
