{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55493bb-5b07-4a93-a674-40980bba06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01cb5e-b5e6-4ad1-848b-00ad69b61d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"[INSERT HERE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1666ce-ce92-445c-941b-68de96a53cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env OPENAI_API_KEY=INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd0832-2aaf-4ed8-b430-4219389a89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    \"reuters\": \"text\",\n",
    "    \"spookyauthor\": \"text\",\n",
    "    \"trustpilot\": \"text\",\n",
    "    \"yelp\": \"review\",\n",
    "    \"reddit-mental-health\":\"text\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9129c63-fd68-4edb-9621-c8e21d90262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "util = {\n",
    "    \"trustpilot\": \"sentiment\",\n",
    "    \"yelp\": \"sentiment\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289b7ac-719d-4a53-93f3-b609922f6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"reuters\": \"author\",\n",
    "    \"spookyauthor\": \"author\",\n",
    "    \"trustpilot\": \"gender\",\n",
    "    \"yelp\": \"user_id\",\n",
    "    \"reddit-mental-health\":\"author_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9599e1-3be0-4a66-a116-8a5d08bf3885",
   "metadata": {},
   "source": [
    "# GEval with Deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009e232-d565-4ca0-b8f9-80364898eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.evaluate import DisplayConfig, evaluate, ErrorConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34352d-b225-4459-bd07-b4a29892f6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10b81c-688b-4fbf-96b6-97f484ba0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency = GEval(\n",
    "    name=\"Fluency\",\n",
    "    criteria=\"Measure how smoothly the text reads, focusing on grammar and syntax.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    strict_mode=False,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad54a6-32d3-4e61-bb60-84b88227f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency = GEval(\n",
    "    name=\"Consistency\",\n",
    "    criteria=\"Ensures the text maintains a uniform style and tone throughout.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    strict_mode=False,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120884d8-591c-4e17-84aa-fdbb6fa7ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarity = GEval(\n",
    "    name=\"Clarity\",\n",
    "    criteria=\"Evaluates how easily the actual output can be understood by the reader.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    strict_mode=False,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43529f-678b-436a-ac3b-8978cbe0bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conciseness = GEval(\n",
    "    name=\"Conciseness\",\n",
    "    criteria=\"Assesses whether the text is free of unnecessary words or details.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    strict_mode=False,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80ddc8-65d0-4498-81af-2579399c30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitiveness = GEval(\n",
    "    name=\"Repetitiveness\",\n",
    "    criteria=\"Checks for redundancy or repeated information in the text.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    strict_mode=False,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb0c5e-567b-4081-bda9-3cd3c9559782",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DisplayConfig(print_results=False, show_indicator=True, verbose_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935b783-3a84-4546-b16a-14e25b6a2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = ErrorConfig(ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e8159-aefa-493d-b037-7e3fae14b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"Fluency\", \"Consistency\", \"Clarity\", \"Conciseness\", \"Repetitiveness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396431fe-6af3-4203-ac9d-fb69ec347259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "for f in fields:\n",
    "    print(f)\n",
    "    if f in log:\n",
    "        continue\n",
    "    \n",
    "    data = pd.read_csv(\"data/datasets/{}.csv\".format(f)).sample(n=100, random_state=42)\n",
    "    cases = []\n",
    "    for x in data[fields[f]].to_list():\n",
    "        cases.append(LLMTestCase(input=x, actual_output=x))\n",
    "\n",
    "    dataset = EvaluationDataset(test_cases=cases)\n",
    "    res = evaluate(test_cases=dataset, metrics=[fluency, consistency, clarity, conciseness, repetitiveness], display_config=display, error_config=error)\n",
    "\n",
    "    results = {m: [] for m in metric_names}\n",
    "    \n",
    "    for r in res.test_results:\n",
    "        for m in r.metrics_data:\n",
    "            results[m.name.split()[0]].append(m.score)\n",
    "\n",
    "    averages = {}\n",
    "    for r in results:\n",
    "        nums = [x for x in results[r] if pd.isnull(x) == False]\n",
    "        if len(nums) > 0:\n",
    "            averages[r] = round(np.mean(nums), 3)\n",
    "        else:\n",
    "            averages[r] = 0\n",
    "\n",
    "    log[f] = averages\n",
    "    print(averages)\n",
    "\n",
    "    with open(\"data/geval_baseline.json\", 'w') as out:\n",
    "        json.dump(log, out, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f205cb-72d4-4262-b324-787f93e18ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in Path(\"data/\").glob(\"*.csv\"):\n",
    "    print(file.stem)\n",
    "    if file.stem in log:\n",
    "        continue\n",
    "    \n",
    "    name = file.stem.split(\"_\")[0]\n",
    "    orig = pd.read_csv(\"data/datasets/{}.csv\".format(name))\n",
    "    priv = pd.read_csv(file).dropna().sample(n=100, random_state=42)\n",
    "    orig = orig.iloc[priv.index]\n",
    "\n",
    "    field = fields[name]\n",
    "    cases = []\n",
    "    for x, y in zip(orig[field].to_list(), priv[field].to_list()):\n",
    "        cases.append(LLMTestCase(input=x, actual_output=y))\n",
    "\n",
    "    dataset = EvaluationDataset(test_cases=cases)\n",
    "    res = evaluate(test_cases=dataset, metrics=[fluency, consistency, clarity, conciseness, repetitiveness], display_config=display, error_config=error)\n",
    "\n",
    "    results = {m: [] for m in metric_names}\n",
    "    \n",
    "    for r in res.test_results:\n",
    "        for m in r.metrics_data:\n",
    "            results[m.name.split()[0]].append(m.score)\n",
    "\n",
    "    averages = {}\n",
    "    for r in results:\n",
    "        nums = [x for x in results[r] if pd.isnull(x) == False]\n",
    "        if len(nums) > 0:\n",
    "            averages[r] = round(np.mean(nums), 3)\n",
    "        else:\n",
    "            averages[r] = 0\n",
    "\n",
    "    log[file.stem] = averages\n",
    "    print(averages)\n",
    "\n",
    "    with open(\"data/geval.json\", 'w') as out:\n",
    "        json.dump(log, out, indent=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
